{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import animation, rc\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/train/binary\"\n",
    "files = glob(f\"{path}/*.csv.gz\")\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "# df = df[df[\"Label\"] == 0]\n",
    "print(df.shape)\n",
    "feature_cols = [c for c in df.columns if c != 'Label']\n",
    "scaler = MinMaxScaler()\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "label_values_counts = len(df[\"Label\"].unique())\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400533f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, target_column):\n",
    "        self.features = df.drop(columns=[target_column]).values.astype(np.float32)\n",
    "        self.labels = df[target_column].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx])\n",
    "        if self.labels is not None:\n",
    "            y = torch.tensor(self.labels[idx])\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_dataset = DataFrameDataset(train_df, target_column='Label')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = DataFrameDataset(val_df, target_column='Label')\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 100\n",
    "\n",
    "# trainval_data = MNIST(\"./test_data\", \n",
    "#                    train=True, \n",
    "#                    download=True, \n",
    "#                    transform=transforms.ToTensor())\n",
    "\n",
    "# train_size = int(len(trainval_data) * 0.8)\n",
    "# val_size = int(len(trainval_data) * 0.2)\n",
    "# train_data, val_data = torch.utils.data.random_split(trainval_data, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_data,\n",
    "#                           batch_size=BATCH_SIZE,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=0)\n",
    "\n",
    "# val_loader = DataLoader(dataset=val_data,\n",
    "#                         batch_size=BATCH_SIZE,\n",
    "#                         shuffle=True,\n",
    "#                         num_workers=0)\n",
    "\n",
    "# print(\"train data size: \",len(train_data))   #train data size:  48000\n",
    "# print(\"train iteration number: \",len(train_data)//BATCH_SIZE)   #train iteration number:  480\n",
    "# print(\"val data size: \",len(val_data))   #val data size:  12000\n",
    "# print(\"val iteration number: \",len(val_data)//BATCH_SIZE)   #val iteration number:  120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(\"images_size:\",images.size())   #images_size: torch.Size([100, 1, 28, 28])\n",
    "print(\"label:\",labels[:10])   #label: tensor([7, 6, 0, 6, 4, 8, 5, 2, 2, 3])\n",
    "\n",
    "# image_numpy = images.detach().numpy().copy()\n",
    "# plt.imshow(image_numpy[0,0,:,:], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, z_dim):\n",
    "    super().__init__()\n",
    "    self.lr = nn.Linear(88, 300)\n",
    "    self.lr2 = nn.Linear(300, 100)\n",
    "    self.lr_ave = nn.Linear(100, z_dim)   #average\n",
    "    self.lr_dev = nn.Linear(100, z_dim)   #log(sigma^2)\n",
    "    self.relu = nn.ReLU()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.lr(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.lr2(x)\n",
    "    x = self.relu(x)\n",
    "    ave = self.lr_ave(x)    #average\n",
    "    log_dev = self.lr_dev(x)    #log(sigma^2)\n",
    "\n",
    "    ep = torch.randn_like(ave)   #平均0分散1の正規分布に従い生成されるz_dim次元の乱数\n",
    "    z = ave + torch.exp(log_dev / 2) * ep   #再パラメータ化トリック\n",
    "    return z, ave, log_dev\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, z_dim):\n",
    "    super().__init__()\n",
    "    self.lr = nn.Linear(z_dim, 100)\n",
    "    self.lr2 = nn.Linear(100, 300)\n",
    "    self.lr3 = nn.Linear(300, 88)\n",
    "    self.relu = nn.ReLU()\n",
    "  \n",
    "  def forward(self, z):\n",
    "    x = self.lr(z)\n",
    "    x = self.relu(x)\n",
    "    x = self.lr2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.lr3(x)\n",
    "    x = torch.sigmoid(x)   #MNISTのピクセル値の分布はベルヌーイ分布に近いと考えられるので、シグモイド関数を適用します。\n",
    "    return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "  def __init__(self, z_dim):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(z_dim)\n",
    "    self.decoder = Decoder(z_dim)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    z, ave, log_dev = self.encoder(x)\n",
    "    x = self.decoder(z)\n",
    "    return x, z, ave, log_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e300dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(predict, target, ave, log_dev):\n",
    "  bce_loss = F.binary_cross_entropy(predict, target, reduction='sum')\n",
    "  kl_loss = -0.5 * torch.sum(1 + log_dev - ave**2 - log_dev.exp())\n",
    "  loss = bce_loss + kl_loss\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = VAE(z_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15], gamma=0.1)\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"ave\": [], \"log_dev\": [], \"z\": [], \"labels\":[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  for i, (x, labels) in enumerate(train_loader):\n",
    "    input = x.to(device).view(-1, 88).to(torch.float32)\n",
    "    output, z, ave, log_dev = model(input)\n",
    "\n",
    "    history[\"ave\"].append(ave)\n",
    "    history[\"log_dev\"].append(log_dev)\n",
    "    history[\"z\"].append(z)\n",
    "    history[\"labels\"].append(labels)\n",
    "    loss = criterion(output, input, ave, log_dev)\n",
    "     \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "     \n",
    "    if (i+1) % 50 == 0:\n",
    "      print(f'\\rEpoch: {epoch+1:3d}, loss: {loss: 04.4f}', end='')\n",
    "    history[\"train_loss\"].append(loss)\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, (x, labels) in enumerate(val_loader):\n",
    "      input = x.to(device).view(-1, 88).to(torch.float32)\n",
    "      output, z, ave, log_dev = model(input)\n",
    "\n",
    "      loss = criterion(output, input, ave, log_dev)\n",
    "      history[\"val_loss\"].append(loss)\n",
    "      \n",
    "    print(f' -> val_loss: {loss: 0.4f}')\n",
    "  \n",
    "  scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_tensor = torch.stack(history[\"train_loss\"])\n",
    "train_loss_np = train_loss_tensor.to('cpu').detach().numpy().copy()\n",
    "plt.plot(train_loss_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_tensor = torch.stack(history[\"val_loss\"])\n",
    "val_loss_np = val_loss_tensor.to('cpu').detach().numpy().copy()\n",
    "plt.plot(val_loss_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c818529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_tensor = torch.cat(history[\"ave\"])\n",
    "log_var_tensor = torch.cat(history[\"log_dev\"])\n",
    "z_tensor = torch.cat(history[\"z\"])\n",
    "labels_tensor = torch.cat(history[\"labels\"])\n",
    "print(ave_tensor.size())   #torch.Size([9600, 100, 2])\n",
    "print(log_var_tensor.size())   #torch.Size([9600, 100, 2])\n",
    "print(z_tensor.size())   #torch.Size([9600, 100, 2])\n",
    "print(labels_tensor.size())   #torch.Size([9600, 100])\n",
    "\n",
    "ave_np = ave_tensor.to('cpu').detach().numpy().copy()\n",
    "log_var_np = log_var_tensor.to('cpu').detach().numpy().copy()\n",
    "z_np = z_tensor.to('cpu').detach().numpy().copy()\n",
    "labels_np = labels_tensor.to('cpu').detach().numpy().copy()\n",
    "print(ave_np.shape)   #(9600, 100, 2)\n",
    "print(log_var_np.shape)   #(9600, 100, 2)\n",
    "print(z_np.shape)   #(9600, 100, 2)\n",
    "print(labels_np.shape)   #(9600, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b82ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_keyword = \"tab10\"\n",
    "cmap = plt.get_cmap(map_keyword)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "for label in range(label_values_counts):\n",
    "    x = z_np[labels_np == label, 0]\n",
    "    y = z_np[labels_np == label, 1]\n",
    "    plt.scatter(x, y, color=cmap(label/label_values_counts), label=label, s=15)\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        plt.annotate(label, xy=(np.mean(x), np.mean(y)), size=20, color=\"black\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 9580\n",
    "plt.figure(figsize=[10,10])\n",
    "for label in range(label_values_counts):\n",
    "  x = z_np[labels_np == label, 0]\n",
    "  y = z_np[labels_np == label, 1]\n",
    "  plt.scatter(x, y, color=cmap(label/label_values_counts), label=label, s=15)\n",
    "  plt.annotate(label, xy=(np.mean(x),np.mean(y)),size=20,color=\"black\")\n",
    "plt.legend(loc=\"upper left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f51124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "\n",
    "label = 0\n",
    "x_zero_mean = np.mean(ave_np[labels_np == label, 0])   #x軸の平均値\n",
    "y_zero_mean = np.mean(ave_np[labels_np == label, 1])   #y軸の平均値\n",
    "z_zero = torch.tensor([x_zero_mean,y_zero_mean], dtype = torch.float32)\n",
    "\n",
    "# output = model.decoder(z_zero)\n",
    "# np_output = output.to('cpu').detach().numpy().copy()\n",
    "# np_image = np.reshape(np_output, (88))\n",
    "# plt.imshow(np_image, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 1\n",
    "x_one_mean = np.mean(ave_np[batch_num:,:,0][labels_np[batch_num:,:] == label])   #x軸の平均値\n",
    "y_one_mean = np.mean(ave_np[batch_num:,:,1][labels_np[batch_num:,:] == label])   #y軸の平均値\n",
    "z_one = torch.tensor([x_one_mean,y_one_mean], dtype = torch.float32)\n",
    "\n",
    "output = model.decoder(z_one)\n",
    "np_output = output.to('cpu').detach().numpy().copy()\n",
    "# np_image = np.reshape(np_output, (28, 28))\n",
    "# plt.imshow(np_image, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46119f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot(frame):\n",
    "#     plt.cla()\n",
    "#     z_zerotoone = ((99 - frame) * z_zero +  frame * z_one) / 99\n",
    "#     output = model.decoder(z_zerotoone)\n",
    "#     np_output = output.detach().numpy().copy()\n",
    "#     np_image = np.reshape(np_output, (28, 28))\n",
    "#     plt.imshow(np_image, cmap='gray')\n",
    "#     plt.xticks([]);plt.yticks([])\n",
    "#     plt.title(\"frame={}\".format(frame))\n",
    "\n",
    "# fig = plt.figure(figsize=(4,4))\n",
    "# ani = animation.FuncAnimation(fig, plot, frames=99, interval=100)\n",
    "# rc('animation', html='jshtml')\n",
    "# ani\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1026f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
